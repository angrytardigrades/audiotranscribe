import whisper
import os
import time
import torch

# âœ… Auto-detect GPU if available, else use CPU
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"\U0001F4E5 Loading Whisper model on {device.upper()}...")

# âœ… Load Whisper model (use "small" for faster speed if needed)
model_size = "medium"
model = whisper.load_model(model_size, device=device)
print(f"âœ… Model '{model_size}' loaded successfully!\n")

# ğŸ™ï¸ **SET YOUR AUDIO FILE PATH HERE**
audio_path = "YOUR AUDIO FILE PATH - EXACT PATH"

# âœ… Check if file exists
if not os.path.exists(audio_path):
    raise FileNotFoundError(f"âŒ Error: Audio file not found at {audio_path}")

# âœ… Create output directory
output_dir = os.path.join(os.path.dirname(audio_path), "OUTPUT PATH")
os.makedirs(output_dir, exist_ok=True)

# âœ… Generate output file name
output_file = os.path.join(output_dir, os.path.basename(audio_path).replace(".mp3", ".txt"))

try:
    print("ğŸ™ï¸ Transcription in progress... (this may take a while)\n")

    # â³ Start timer
    start_time = time.time()

    # âœ… Transcribe audio (no timestamps)
    result = model.transcribe(audio_path, word_timestamps=False, verbose=True)  

    # â±ï¸ End timer
    elapsed_time = time.time() - start_time

    # âœ… Ensure transcription has content
    if "text" not in result or not result["text"].strip():
        raise ValueError("âŒ No transcription found! Check if the audio has clear speech.")

    # âœ… Save transcription (no timestamps)
    print(f"ğŸ“„ Saving transcription to: {output_file}")
    with open(output_file, "w", encoding="utf-8") as f:
        f.write(result["text"] + "\n")

    print(f"\nâœ… Transcription finished in {elapsed_time:.2f} seconds.")
    print(f"ğŸ“‚ Transcription saved at: {output_file}")

except Exception as e:
    print(f"\nâŒ Error during transcription: {e}")
